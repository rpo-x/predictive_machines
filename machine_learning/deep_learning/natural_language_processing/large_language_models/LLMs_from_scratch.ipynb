{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inspired by \"Language Modeling from Scratch\" CS336 2025 (Stanford Univ.)",
   "id": "e366526efc1aef4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Large language models (LLMs) below some parameter threshold (e.g., 1B parameters) are potentially not representative of LLMs used in the private sector. E.g., GPT-4 has 1.8T parameters and costs 100k USD to train; xAI's Colossus cluster has 150k H100 GPUs and 50k H200 GPUs (06/2025); there is an unpredictable phenomenon present in LLMs referred to as emergent abilities and a scaling law about how FLOPs distribute across components as parameter count increases. ",
   "id": "4681cf6d9189c6ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "CONTINUE...",
   "id": "460d787fd894181"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
