{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notebook [3]",
   "id": "60b2ef0b5947c0d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Perfect. Let's take a look at an example of machine translation, a form of text generation.\n",
    "\n",
    "In machine translation (obviously) we map an input text (a sequence of tokens) from a source language to an output text (a sequence of tokens) in a target language. As an example, translate the sentence \"Tim Cook presented the new iPhone in Las Vegas on Tuesday.\" from English to German. \n",
    "\n",
    "For this, we could use the model Helsinki-NLP/opus-mt-en-de, a transformer encoder–decoder trained for English–German translation on the dataset OPUS. OPUS is a collection of parallel corpora, where each example is expressed in two languages e.g., an English sentence paired with its human translation in German. To be clear here, Helsinki-NLP/opus-mt-en-de is a task-specific neural machine translation model, not a general-purpose LLM.\n",
    "\n",
    "Let’s code this."
   ],
   "id": "2d15d27dd5ee7da5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T09:55:51.854471Z",
     "start_time": "2026-01-09T09:55:50.211316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    translator = pipeline(\n",
    "        task=\"translation\",\n",
    "        model=MODEL_NAME,\n",
    "        framework=\"pt\",\n",
    "    )\n",
    "\n",
    "    text = \"Tim Cook presented the new iPhone in Las Vegas on Tuesday.\"\n",
    "    result = translator(text)  # pipeline returns a list of dictionaries\n",
    "\n",
    "    for output in result:\n",
    "        print(f\"Translation: {output['translation_text']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "cf668251fbaa2166",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Tim Cook präsentierte das neue iPhone am Dienstag in Las Vegas.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great! To translate the generated German sentence back to English, we require a separate model (at least in classical machine translation). LLMs learn translation implicitly from multilingual data, but (for now) we restrict ourselves to classical machine translation where this is not the case.\n",
    "\n",
    "In classical machine translation models are directional by construction: each model is trained on sentence pairs in a fixed direction and learns a specific conditional distribution e.g., $P(\\text{German} \\mid \\text{English})\n",
    "$. The inverse distribution $P(\\text{English} \\mid \\text{German})$ is a different learning problem and therefore requires a separately trained model.\n",
    "\n",
    "Let's code this using the model Helsinki-NLP/opus-mt-de-en. (de-en instead of en-de)."
   ],
   "id": "eed77b92d0cef01d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T09:58:17.966019Z",
     "start_time": "2026-01-09T09:58:16.323601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    translator = pipeline(\n",
    "        task=\"translation\",\n",
    "        model=MODEL_NAME,\n",
    "        framework=\"pt\",\n",
    "    )\n",
    "\n",
    "    text = \"Tim Cook präsentierte das neue iPhone am Dienstag in Las Vegas.\"\n",
    "    result = translator(text)  # pipeline returns a list of dictionaries\n",
    "\n",
    "    for output in result:\n",
    "        print(f\"Translation: {output['translation_text']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "50dda9fcf5fcf7a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation: Tim Cook presented the new iPhone on Tuesday in Las Vegas.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nice. There is an additional and potentially obvious insight here. \n",
    "\n",
    "As explained above, in classical machine translation each model learns a direction-specific conditional distribution. Consequently, translating a sentence from English to German and then back to English does not guarantee recovery of the original sentence (i.e., the exact sequence of tokens). During back-translation, the model is given only the German sentence and generates an English sentence according to the conditional distribution $P(\\text{English} \\mid \\text{German})$. Because the original English wording is not recoverable from the German sentence alone, the model produces one of several equally valid English realisations (realisation is a concrete surface form, represented by a specific sequence of tokens)."
   ],
   "id": "4eae70c9ab710358"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Continue.",
   "id": "81eef68e5862ead6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
