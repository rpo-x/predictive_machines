{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Inspired by \"Deep Learning for Computer Vision\" CS231N 2025 (Stanford Univ.)",
   "id": "5e56ca5171641df0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In image classification, a system takes an input image and assigns one or more of several predefined labels (e.g., \"dog\" or \"cat\"). If we assign one label to an image, it is single-label classification; if we assign multiple labels, it is multi-label classification. I deliberately chose the word \"system\" to refer to the complete computational setup, including the algorithm, its trained parameters, the runtime environment, input/output interfaces, and supporting infrastructure.  \n",
    "\n",
    "On a computer, images are typically represented as tensors (multidimensional arrays) of integers.  \n",
    "\n",
    "For example, an 800×600 color (RGB) image is represented as a 3rd-order tensor with shape (800, 600, 3), where each value is an unsigned 8-bit integer in the range `[0, 255]`. The third dimension (size 3) corresponds to the RGB color channels (Red, Green, Blue). There are, of course, multiple ways to represent images (e.g., grayscale, CMYK, floating-point values normalized to `[0, 1]`, or other color spaces besides RGB)."
   ],
   "id": "67e2fcfb71723115"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:24:42.739186Z",
     "start_time": "2025-11-12T13:24:42.724260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3rd-order tensor: 2x2 RGB color image\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "rgb_image = np.random.randint(0, 256, size=(2, 2, 3), dtype=np.uint8)\n",
    "print(\"RGB image shape:\", rgb_image.shape)   # (2, 2, 3)\n",
    "print(\"RGB image dtype:\", rgb_image.dtype)   # uint8\n",
    "print(rgb_image)"
   ],
   "id": "6c9e348493ab9f22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB image shape: (2, 2, 3)\n",
      "RGB image dtype: uint8\n",
      "[[[102 220 225]\n",
      "  [ 95 179  61]]\n",
      "\n",
      " [[234 203  92]\n",
      "  [  3  98 243]]]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The example image above is a 2×2 pixel RGB image (4 pixels total). The first element of the first row in the 3rd-order tensor represents the RGB values of the top-left pixel at position (0,0): `[102, 220, 225]`, where 102 is the red channel intensity, 220 is the green channel intensity, and 225 is the blue channel intensity.",
   "id": "1ed8fada384189ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For a black-and-white (grayscale) image, only one channel is needed. It can be represented either as:\n",
    "- a 2nd-order tensor of shape e.g., `(800, 600)`, or  \n",
    "- a 3rd-order tensor of shape e.g., `(800, 600, 1)` (to maintain consistency with color images)."
   ],
   "id": "fa1bd050bd82ccbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:25:56.045049Z",
     "start_time": "2025-11-12T13:25:56.041571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2nd-order tensor: 2x2 grayscale image\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "grayscale_image = np.array([[100, 200], [50, 150]], dtype=np.uint8)\n",
    "print(\"Grayscale image shape:\", grayscale_image.shape)  # (2, 2)\n",
    "print(\"Grayscale image dtype:\", grayscale_image.dtype)  # uint8\n",
    "print(grayscale_image)"
   ],
   "id": "8b177191052affe8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayscale image shape: (2, 2)\n",
      "Grayscale image dtype: uint8\n",
      "[[100 200]\n",
      " [ 50 150]]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "25667df3a4ef36d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are two main approaches to image classification:\n",
    "\n",
    "1. Hard-coded rules (actually tried in early computer vision). You can check this out, here engineers manually defined rules to detect features like edges, corners, and textures.\n",
    "2. Machine learning (ML) (data-driven method). This is the approach we focus on...\n",
    "\n",
    "In ML-based image classification, we basically follow three steps:\n",
    "\n",
    "1. Collect a labeled dataset (e.g., ImageNet)  \n",
    "2. Train a classifier using ML algorithms  \n",
    "3. Evaluate the model on new, unseen images  \n",
    "\n",
    "Here is a simplified illustration (in pseudo code):"
   ],
   "id": "84051e51be7bd0b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:27:16.069464Z",
     "start_time": "2025-11-12T13:27:16.065795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Collect a labeled dataset (e.g., ImageNet)\n",
    "images = None\n",
    "labels = None\n",
    "\n",
    "# 2. Train a classifier using ML algorithms\n",
    "def train(images, labels):\n",
    "    # Machine learning!\n",
    "    return \"model\"\n",
    "\n",
    "model = train(images, labels)\n",
    "\n",
    "# 3. Evaluate on new, unseen images\n",
    "def predict(model, image):\n",
    "    # Use model to predict label for the input image\n",
    "    return \"cat\"\n",
    "\n",
    "image = None  # The image you want to classify\n",
    "label = predict(model, image)\n",
    "print(label)  # Output: cat"
   ],
   "id": "bdb4e7669bd77e34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A classic and simple baseline for image classification is the nearest neighbor method (also known as k-nearest neighbors, or k-NN). We will explore the \"k\" part later.\n",
    "\n",
    "For this model, the train() function simply stores all training images and their labels in memory (as 2nd or 3rd-order tensors, as described before). It is important to understand that this is not learning. No patterns or features are extracted; the model simply stores the raw training examples (as tensors).  \n",
    "The predict() function assigns to a new image the label of the most similar training image, where similarity is measured by a distance function that computes a numerical value between the new image and each stored training image.  \n",
    "\n",
    "There are many distance metrics used in k-NN. Two of the most common for images are:\n",
    "\n",
    "- L1 distance (Manhattan distance):  \n",
    "  Sum the absolute differences $|I_1^p - I_2^p|$ over every pixel $p$ in the image.  \n",
    "  $$\n",
    "  d_1(I_1, I_2) = \\sum_{p} |I_1^p - I_2^p|\n",
    "  $$\n",
    "\n",
    "- L2 distance (Euclidean distance):  \n",
    "  Sum the squared differences $(I_1^p - I_2^p)^2$ over every pixel $p$, then take the square root.  \n",
    "  $$\n",
    "  d_2(I_1, I_2) = \\sqrt{ \\sum_{p} (I_1^p - I_2^p)^2 }\n",
    "  $$\n",
    "\n",
    "Just as a side note, many other distance metrics exist (e.g., cosine similarity, Hamming distance, Minkowski distances of order $p$), but L1 and L2 remain the standard default choices in raw-pixel k-NN."
   ],
   "id": "32e5a5d933838761"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To make this practical, suppose we have only two training images (one labeled \"cat\" and one labeled \"dog\").  \n",
    "\n",
    "Using 1-nearest neighbors (i.e., pick the single most similar training example) with L1 distance, the prediction works as follows:"
   ],
   "id": "590746e5384bf9ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:53:16.102301Z",
     "start_time": "2025-11-12T13:53:16.094162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training data (each entry: (image_array, label))\n",
    "training_data = [\n",
    "    (np.array([[10, 20], [30, 40]]), \"dog\"),\n",
    "    (np.array([[100, 110], [120, 130]]), \"cat\"),\n",
    "]\n",
    "\n",
    "# Image to classify\n",
    "new_image = np.array([[12, 22], [28, 38]])\n",
    "\n",
    "best_label = None\n",
    "best_distance = np.inf\n",
    "\n",
    "for image, label in training_data:\n",
    "    # L1 distance (sum of absolute pixel-wise differences)\n",
    "    distance = np.abs(image - new_image).sum()\n",
    "    print(f\"Distance to {label}: {distance}\")\n",
    "\n",
    "    if distance < best_distance:\n",
    "        best_distance = distance\n",
    "        best_label = label\n",
    "\n",
    "print(f\"\\nPredicted label: {best_label}\")\n"
   ],
   "id": "d921bb4a68c4c7fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to dog: 8\n",
      "Distance to cat: 360\n",
      "\n",
      "Predicted label: dog\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So this was quite straightforward, wasn't it? We took the new image (represented as a tensor), computed the L1 distance to each training image, and assigned the label of the one with the smallest distance. It's that simple.  \n",
    "\n",
    "In practice, k-nearest neighbors uses k > 1 (rather than simply copying the label of the single closest neighbor). For example, with six training images (three labeled \"dog\" and three labeled \"cat\"), we can set k = 3. We typically choose an odd k (like 3, 5, or 7) to avoid ties in voting. The algorithm computes the L1 distance from the new image to all six training examples, sorts them by distance, selects the three closest, and predicts the majority label among them.\n",
    "\n",
    "This process is just as simple and clearly shows that k-NN does not learn. It does not extract patterns, build rules, or adjust internal parameters during training. It simply stores all training data in memory and makes predictions by comparing new inputs to these stored examples at runtime."
   ],
   "id": "15c7508ae336e4e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:23:58.426216Z",
     "start_time": "2025-11-12T14:23:58.418348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Training data: (each entry: (image_array, label))\n",
    "training_data = [\n",
    "    (np.array([[12, 18], [31, 45]]), \"dog\"),\n",
    "    (np.array([[15, 22], [29, 41]]), \"dog\"),\n",
    "    (np.array([[9, 19], [35, 43]]), \"dog\"),\n",
    "    (np.array([[95, 108], [118, 132]]), \"cat\"),\n",
    "    (np.array([[102, 111], [121, 138]]), \"cat\"),\n",
    "    (np.array([[98, 105], [119, 128]]), \"cat\"),\n",
    "]\n",
    "\n",
    "# Image to classify\n",
    "new_image = np.array([[14, 21], [32, 44]])\n",
    "\n",
    "\n",
    "# k-NN settings\n",
    "k = 3\n",
    "\n",
    "# L1 distance (sum of absolute pixel-wise differences) and collect (distance, label) pairs\n",
    "distances = [(np.abs(img - new_image).sum(), label) for img, label in training_data]\n",
    "\n",
    "# Sort by distance (ascending)\n",
    "distances.sort(key=lambda x: x[0])\n",
    "\n",
    "# Get k nearest labels\n",
    "nearest_labels = [label for _, label in distances[:k]]\n",
    "\n",
    "# Predict via majority vote\n",
    "predicted = Counter(nearest_labels).most_common(1)[0][0]\n",
    "\n",
    "# Output\n",
    "print(\"Distances (sorted):\")\n",
    "for dist, label in distances:\n",
    "    print(f\"  {label}: {dist}\")\n",
    "\n",
    "print(f\"\\n{k} nearest labels: {nearest_labels}\")\n",
    "print(f\"Predicted: {predicted}\")\n"
   ],
   "id": "1d9ee95659a93e4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances (sorted):\n",
      "  dog: 7\n",
      "  dog: 8\n",
      "  dog: 11\n",
      "  cat: 339\n",
      "  cat: 342\n",
      "  cat: 361\n",
      "\n",
      "3 nearest labels: ['dog', 'dog', 'dog']\n",
      "Predicted: dog\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Perfect!.",
   "id": "a305af7f7264bfae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:22:47.166343Z",
     "start_time": "2025-11-12T15:22:47.162228Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d8f63777261e66ae",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "67b5ce1076704bcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e8c06da3be94608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eac725a556eca6f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f6d70db97cbdde56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fdd1fcacdb242616"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "af6c015db8debc56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "891d948deacce95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "EXTRA MATERIAL:  \n",
    "\n",
    "There are multiple challenges in image classification. For example:\n",
    "\n",
    "- Viewpoint variation – the same object looks completely different from different angles  \n",
    "- Illumination changes – lighting conditions alter pixel (RGB) values significantly (e.g., sunlight vs. indoor)  \n",
    "- Background clutter – the background resembles the object, making it hard to isolate (e.g., a camouflaged insect)  \n",
    "- Occlusion – parts of the object are hidden by other objects (e.g., only ears or tail visible)  \n",
    "- Deformation – non-rigid objects change shape (e.g., a cat curled up, stretched, or in a weird pose)  \n",
    "- Intra-class variation – objects in the same class vary greatly in appearance (e.g., different breeds or sizes)  \n",
    "- Context – classification depends on surrounding objects or scene"
   ],
   "id": "297194f775ded490"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
